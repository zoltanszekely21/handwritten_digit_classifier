{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handwritten_digit_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6khAsCDI6oZ2"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from time import time\r\n",
        "from torchvision import datasets, transforms\r\n",
        "from torch import nn, optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvNil1pj6t6F"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\r\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\r\n",
        "                              ])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHFH_HBU6uY4"
      },
      "source": [
        "trainset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=True, transform=transform)\r\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\r\n",
        "\r\n",
        "testset = datasets.MNIST(r'..\\input\\MNIST', download=True, train=False, transform=transform)\r\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvOlTcVv67nk"
      },
      "source": [
        "dataiter = iter(trainloader) # creating a iterator\r\n",
        "images, labels = dataiter.next() # creating images for image and lables for image number (0 to 9) \r\n",
        "\r\n",
        "print(images.shape)\r\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erjYtUSN6_nq"
      },
      "source": [
        "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FcbJvDm7ASK"
      },
      "source": [
        "figure = plt.figure()\r\n",
        "num_of_images = 60\r\n",
        "for index in range(1, num_of_images + 1):\r\n",
        "    plt.subplot(6, 10, index)\r\n",
        "    plt.axis('off')\r\n",
        "    plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRIkTXsG7Da9"
      },
      "source": [
        "# Model creation with neural net Sequential model\r\n",
        "model=nn.Sequential(nn.Linear(784,128), # 1 layer:- 784 input 128 o/p\r\n",
        "                    nn.ReLU(),          # Defining Regular linear unit as activation\r\n",
        "                    nn.Linear(128,64),  # 2 Layer:- 128 Input and 64 O/p\r\n",
        "                    nn.Tanh(),          # Defining Regular linear unit as activation\r\n",
        "                    nn.Linear(64,10),   # 3 Layer:- 64 Input and 10 O/P as (0-9)\r\n",
        "                    nn.LogSoftmax(dim=1) # Defining the log softmax to find the probablities for the last output unit\r\n",
        "                  ) \r\n",
        "\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV-f4sjN7HK8"
      },
      "source": [
        "# defining the negative log-likelihood loss for calculating loss\r\n",
        "criterion = nn.NLLLoss() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4I_mItK7IfN"
      },
      "source": [
        "images, labels = next(iter(trainloader))\r\n",
        "images = images.view(images.shape[0], -1)\r\n",
        "\r\n",
        "logps = model(images) #log probabilities\r\n",
        "loss = criterion(logps, labels) #calculate the NLL-loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG4SJUhm7Kzz"
      },
      "source": [
        "print('Before backward pass: \\n', model[0].weight.grad)\r\n",
        "loss.backward() # to calculate gradients of parameter \r\n",
        "print('After backward pass: \\n', model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODjW0aJ97Mwi"
      },
      "source": [
        "# defining the optimiser with stochastic gradient descent and default parameters\r\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
        "\r\n",
        "print('Initial weights - ', model[0].weight)\r\n",
        "\r\n",
        "images, labels = next(iter(trainloader))\r\n",
        "images.resize_(64, 784)\r\n",
        "\r\n",
        "# Clear the gradients, do this because gradients are accumulated\r\n",
        "optimizer.zero_grad()\r\n",
        "\r\n",
        "# Forward pass\r\n",
        "output = model(images)\r\n",
        "loss = criterion(output, labels)\r\n",
        "# the backward pass and update weights\r\n",
        "loss.backward()\r\n",
        "print('Gradient -', model[0].weight.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8hIVgyy7QSO"
      },
      "source": [
        "time0 = time()\r\n",
        "epochs = 15 # total number of iteration for training\r\n",
        "running_loss_list= []\r\n",
        "epochs_list = []\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "    running_loss = 0\r\n",
        "    for images, labels in trainloader:\r\n",
        "        # Flatenning MNIST images with size [64,784]\r\n",
        "        images = images.view(images.shape[0], -1) \r\n",
        "    \r\n",
        "        # defining gradient in each epoch as 0\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # modeling for each image batch\r\n",
        "        output = model(images)\r\n",
        "        \r\n",
        "        # calculating the loss\r\n",
        "        loss = criterion(output, labels)\r\n",
        "        \r\n",
        "        # This is where the model learns by backpropagating\r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        # And optimizes its weights here\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        # calculating the loss\r\n",
        "        running_loss += loss.item()\r\n",
        "        \r\n",
        "    else:\r\n",
        "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\r\n",
        "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70SbHbIV7SJj"
      },
      "source": [
        "def classify(img, ps):\r\n",
        "    ''' \r\n",
        "    Function for viewing an image and it's predicted classes.\r\n",
        "    '''\r\n",
        "    ps = ps.data.numpy().squeeze()\r\n",
        "\r\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\r\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\r\n",
        "    ax1.axis('off')\r\n",
        "    ax2.barh(np.arange(10), ps)\r\n",
        "    ax2.set_aspect(0.1)\r\n",
        "    ax2.set_yticks(np.arange(10))\r\n",
        "    ax2.set_yticklabels(np.arange(10))\r\n",
        "    ax2.set_title('Class Probability')\r\n",
        "    ax2.set_xlim(0, 1.1)\r\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KroC4mNw7TbP"
      },
      "source": [
        "images, labels = next(iter(testloader))\r\n",
        "# replace trainloader to check training accuracy.\r\n",
        "\r\n",
        "img = images[0].view(1, 784)\r\n",
        "# Turn off gradients to speed up this part\r\n",
        "with torch.no_grad():\r\n",
        "    logpb = model(img)\r\n",
        "\r\n",
        "# Output of the network are log-probabilities, need to take exponential for probabilities\r\n",
        "pb = torch.exp(logpb)\r\n",
        "probab = list(pb.numpy()[0])\r\n",
        "print(\"Predicted Digit =\", probab.index(max(probab)))\r\n",
        "classify(img.view(1, 28, 28), pb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlv5tHg27U6B"
      },
      "source": [
        "correct_count, all_count = 0, 0\r\n",
        "for images,labels in testloader:\r\n",
        "  for i in range(len(labels)):\r\n",
        "    img = images[i].view(1, 784)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        logps = model(img)\r\n",
        "\r\n",
        "    ps = torch.exp(logps)\r\n",
        "    probab = list(ps.numpy()[0])\r\n",
        "    pred_label = probab.index(max(probab))\r\n",
        "    true_label = labels.numpy()[i]\r\n",
        "    if(true_label == pred_label):\r\n",
        "      correct_count += 1\r\n",
        "    all_count += 1\r\n",
        "\r\n",
        "print(\"Number Of Images Tested =\", all_count)\r\n",
        "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr1sEi_X7WM4"
      },
      "source": [
        "torch.save(model, 'path/to/save/my_mnist_model.pt') # or .pth extension"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}